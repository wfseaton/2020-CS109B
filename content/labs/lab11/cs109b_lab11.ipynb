{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "## Lab 11: RL\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2020**<br>\n",
    "**Instructors:** Mark Glickman, Pavlos Protopapas, and Chris Tanner<br>\n",
    "**Lab Instructors:** Chris Tanner and Eleni Angelaki Kaxiras<br>\n",
    "**Content:** Srivatsan Srinivasan, Pavlos Protopapas, Chris Tanner \n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we are going to work with OpenAIgym's FrozenLake environment. The details of the environment can be found in the link https://gym.openai.com/envs/FrozenLake-v0/. \n",
    "\n",
    "Please visit http://gym.openai.com/docs/ for full documentation!\n",
    "\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. \n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, fall to your doom\n",
    "- G: goal, where the frisbee is located\n",
    "\n",
    "SFFF  <br> \n",
    "FHFH <br> \n",
    "FFFH  <br> \n",
    "HFFG <br> \n",
    "\n",
    "Expected actions are Left(0), Right(1), Down(2), Up(3). \n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gym.envs.registration import register\n",
    "register(id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLake8x8NotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*hint:* If you receive an error message while registering the above env the second time you run this cell again, ignore the error message or restart the kernel. \n",
    "\n",
    "Throughout the assignment, use only the environments we registered in the previous cells:\n",
    "- `FrozenLake8x8NotSlippery-v0`\n",
    "- `FrozenLakeNotSlippery-v0` \n",
    "\n",
    "Even though the original problem description has slippery environment, we are working in a non-slippery environment. In our environment, if you go right, you only go right whereas in the original environment, if you intend to go right, you can go right, up or down with 1/3 probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#Change environment to FrozenLake8x8 to see grid.\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('FrozenLake8x8NotSlippery-v0')\n",
    "\n",
    "print(env.observation_space.n)\n",
    "\n",
    "#Both the grids look like as follows.\n",
    "'''\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ]'''\n",
    "\n",
    "#env.render() prints the frozenlake with an indicator showing where the agent is. You can use it for debugging.\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "def choose_action(state):\n",
    "    return np.random.choice(np.array([0,1,2,3]))\n",
    "\n",
    "def learn(s, s1, r, a):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  0  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  1  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  3  State 1 :  2  Reward :  0.0 Done :  False\n",
      "State :  2  Action :  1  State 1 :  6  Reward :  0.0 Done :  False\n",
      "State :  6  Action :  0  State 1 :  10  Reward :  0.0 Done :  False\n",
      "State :  10  Action :  0  State 1 :  9  Reward :  0.0 Done :  False\n",
      "State :  9  Action :  1  State 1 :  8  Reward :  0.0 Done :  False\n",
      "State :  8  Action :  0  State 1 :  8  Reward :  0.0 Done :  False\n",
      "State :  8  Action :  1  State 1 :  9  Reward :  0.0 Done :  False\n",
      "State :  9  Action :  2  State 1 :  10  Reward :  0.0 Done :  False\n",
      "State :  10  Action :  2  State 1 :  14  Reward :  0.0 Done :  False\n",
      "State :  14  Action :  2  State 1 :  14  Reward :  0.0 Done :  False\n",
      "State :  14  Action :  1  State 1 :  13  Reward :  0.0 Done :  False\n",
      "State :  13  Action :  2  State 1 :  14  Reward :  0.0 Done :  False\n",
      "State :  14  Action :  0  State 1 :  14  Reward :  0.0 Done :  False\n",
      "State :  14  Action :  3  State 1 :  13  Reward :  0.0 Done :  False\n",
      "State :  13  Action :  0  State 1 :  9  Reward :  0.0 Done :  False\n",
      "State :  9  Action :  2  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  0  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  1  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  2  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  0  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  2  State 1 :  8  Reward :  0.0 Done :  False\n",
      "State :  8  Action :  0  State 1 :  8  Reward :  0.0 Done :  False\n",
      "State :  8  Action :  3  State 1 :  9  Reward :  0.0 Done :  False\n",
      "State :  9  Action :  1  State 1 :  10  Reward :  0.0 Done :  False\n",
      "State :  10  Action :  3  State 1 :  6  Reward :  0.0 Done :  False\n",
      "State :  6  Action :  3  State 1 :  7  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  3  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  2  Reward :  0.0 Done :  False\n",
      "State :  2  Action :  1  State 1 :  6  Reward :  0.0 Done :  False\n",
      "State :  6  Action :  0  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  1  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  1  State 1 :  2  Reward :  0.0 Done :  False\n",
      "State :  2  Action :  3  State 1 :  3  Reward :  0.0 Done :  False\n",
      "State :  3  Action :  2  State 1 :  7  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  2  Reward :  0.0 Done :  False\n",
      "State :  2  Action :  0  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  4  Reward :  0.0 Done :  False\n",
      "State :  4  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  2  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  1  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  0  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  3  State 1 :  0  Reward :  0.0 Done :  False\n",
      "State :  0  Action :  3  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  0  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n",
      "State :  0  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  3  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  2  State 1 :  1  Reward :  0.0 Done :  False\n",
      "State :  1  Action :  1  State 1 :  5  Reward :  0.0 Done :  True\n",
      "Episode Over\n",
      "Fell into hole with reward  0.0\n"
     ]
    }
   ],
   "source": [
    "# Set learning parameters\n",
    "################\n",
    "\n",
    "# num_episodes = 2000\n",
    "# epsilon = 0.0\n",
    "# max_steps = 100\n",
    "# lr_rate = 0.0\n",
    "# gamma = 0.0\n",
    "# rList = []\n",
    "\n",
    "num_episodes = 10\n",
    "max_iter_per_episode = 20\n",
    "for i in range(num_episodes):\n",
    "    iter = 0\n",
    "            \n",
    "    #Reset environment and get an initial state - should be done at start of each episode.\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    while iter < max_iter_per_episode:\n",
    "        iter+=1\n",
    "        #Choose an action\n",
    "        a = choose_action(s)\n",
    "        # env.step() gives you next state, reward, done(whether the episode is over)\n",
    "        # s1 - new state, r-reward, d-whether you are done or not\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        print('State : ',s, ' Action : ', a, ' State 1 : ', s1, ' Reward : ',r, 'Done : ', d)\n",
    "        \n",
    "        learn(s, s1, r, a)\n",
    "        \n",
    "        if d:\n",
    "            print('Episode Over')\n",
    "            if r != 1:\n",
    "                print('Fell into hole with reward ', r)            \n",
    "            break\n",
    "        s = s1\n",
    "    if r==1:\n",
    "        print(i)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
